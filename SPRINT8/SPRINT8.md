# üîñ **SUM√ÅRIO**

Nesta p√°gina encontra-se 4 T√≥picos Principais, s√£o estes:

+ **[ATIVIDADE 1 - Exerc√≠cios TMDB](#1-atividade-1---exerc√≠cio-tmdb)**
+ **[ATIVIDADE 2 - Desafio](#2-atividade-2---desafio-pt2)**
+ **[ATIVIDADE 3 - Exerc√≠cios Gera√ß√£o de Massa de Dados](#3-atividade-3---exerc√≠cios-gera√ß√£o-de-massa-de-dados)**
+ **[ATIVIDADE 4 - Exerc√≠cios Apache Spark](#4-atividade-4---exerc√≠cios-apache-spark)**
+ **[Certifica√ß√£o](#üìù-certifica√ß√£o).**

---

&nbsp;

# 1. ATIVIDADE 1 - Exerc√≠cio TMDB

Nesta atividades iremos criar um processo de extra√ß√£o de dados da API do TMDB utilizando servi√ßos da AWS.

Uma API (Application Programming Interface) √© um conjunto de regras, protocolos e ferramentas que permitem que diferentes sistemas de software se comuniquem e troquem informa√ß√µes de forma eficiente e padronizada.

Em outras palavras, uma API √© uma interface que permite que desenvolvedores de software acessem dados ou funcionalidades de um sistema ou aplicativo sem precisar conhecer todos os detalhes internos do sistema. Deste modo, a API fornece uma maneira de acessar esses recursos de forma program√°tica, geralmente usando requisi√ß√µes HTTP (Hypertext Transfer Protocol) para recuperar e/ou enviar dados.

A API TMDB √© uma API RESTful, o que significa que os dados s√£o acessados atrav√©s de URLs que correspondem a recursos espec√≠ficos. Os desenvolvedores podem acessar informa√ß√µes de busca, detalhes de filmes e programas de TV, imagens e informa√ß√µes relacionadas a g√™neros e classifica√ß√µes.

+ Para saber mais sobre a API do The Movie Database, visite o site oficial (https://www.themoviedb.org/documentation/api)  e verifique os termos de uso (https://www.themoviedb.org/documentation/api/terms-of-use) .

+ Para a realiza√ß√£o das atividades √© de grande import√¢ncia que voc√™ fa√ßa a leitura da documenta√ß√£o dispon√≠vel em https://developers.themoviedb.org/3/movies/get-movie-details .

Esta atividade corresponde a um laborat√≥rio. N√£o esperamos que voc√™ registre resposta neste espa√ßo. Contudo, deves adicionar o c√≥digo-fonte produzido ao seu reposit√≥rio no Github. Lembre-se de remover suas credenciais de acesso antes de efetuar commit.

Perguntas dessa tarefa:

+ **Etapa 1 -  Criando sua conta no TMDB**

Ser√° preciso criar uma conta no porta do TMDB para, ap√≥s, solicitar as chaves de acesso para uso da API.

Os passos s√£o:

- Acessar o portal pelo link https://www.themoviedb.org/

- Clique no bot√£o Junte-se ao TMDB na barra de navega√ß√£o no topo da p√°gina

- Preencha o formul√°rio de inscri√ß√£o com as informa√ß√µes solicitadas e clique em Registrar. Utilize seu e-mail pessoal neste passo.

-  Voc√™ ir√° receber um e-mail de confirma√ß√£o. Siga o processo solicitado

- Fa√ßa login em sua nova conta no TMDB e v√° para o seu perfil, clicando no √≠cone de usu√°rio no canto superior direito da p√°gina

- Clique na guia  Vis√£o geral, op√ß√£o Editar Perfil

- Clique no menu API, √† esquerda. A seguir, na op√ß√£o Criar, escolhendo o tipo Developer

- Aceite os termos e preencha o formul√°rio com as informa√ß√µes solicitadas sobre a aplica√ß√£o.

----- Em Tipo de Uso, informe Pessoal

----- Em URL, voc√™ pode informar um endere√ßo fict√≠cio.

----- No Resumo, informe que o objetivo √© para estudos

Etapa 2 - Testando rapidamente as credenciais e a biblioteca



Uma vez que voc√™ tenha sua chave de API, voc√™ pode fazer solicita√ß√µes √† API usando a seguinte estrutura de URL:

https://api.themoviedb.org/3/{endpoint}?api_key={sua_chave_de_api}&{par√¢metros_opcionais}

Onde {endpoint} √© o recurso que voc√™ deseja acessar (por exemplo, movie/{movie_id} para obter detalhes de um filme espec√≠fico) e {par√¢metros_opcionais} s√£o quaisquer par√¢metros adicionais que voc√™ deseje incluir na solicita√ß√£o (por exemplo, language=pt-BR para obter informa√ß√µes em portugu√™s).

Abaixo exemplo de c√≥digo Python:

```python
import requests
import pandas as pd

from IPython.display import display

api_key = "SUA CHAVE"

url = f"https://api.themoviedb.org/3/movie/top_rated?api_key={api_key}&language=pt-BR"

response = requests.get(url)
data = response.json()

filmes = []

for movie in data['results']:
df = {'Titulo': movie['title'],
'Data de lan√ßamento': movie['release_date'],
'Vis√£o geral': movie['overview'],
'Votos': movie['vote_count'],
'M√©dia de votos:': movie['vote_average']}

filmes.append(df)

df = pd.DataFrame(filmes)
display(df)
```

**RESPOSTA:**

<div align="center">

![Tarefa 1](<Images Sprint8/Tarefa1.png>)

</div>

# 2. ATIVIDADE 2 - Desafio PT.2

Etapa 2 - Ingest√£o streaming/micro batch

Nesta etapa do desafio capturaremos dados do TMDB, utilizando AWS Lambda para realizar chamadas de API. Os dados coletados devem ser persistidos no Amazon S3, na camada RAW, mantendo o formato da origem (JSON) e, se poss√≠vel, agrupando os registros em arquivos com, no m√°ximo, 100 registros cada.

O objetivo desta etapa √© complementar os dados dos Filmes e Series, carregados na Etapa 1, com dados oriundos do TMDB. Opcionalmente, voc√™ pode complementar com mais dados de outra API de sua escolha.

Importante:

+ Os arquivos CSVs carregados na Etapa 1 (Sprint 7) n√£o devem ser modificados.

+ Os novos dados devem ser complementares aos dados do CSV. Ou seja, devem existir informa√ß√µes novas sobre os dados do CSV.

+ N√£o √© necess√°rio realizar tratamento dos dados externos, o m√°ximo que pode ser feito √© o agrupamento de dados.

+ Cuidado para os arquivos JSON gerados n√£o serem maior do que 10 MB.

+ N√£o agrupe JSON com estruturas diferentes.

+ Os IDs do IMDB presentes nos arquivos CSV podem ser utilizados em pesquisas  no TMDB.

+ Se voc√™ escolher fazer sobre um filme ou uma trilogia espec√≠fica, considere utilizar pelo menos 4 m√©todos de API diferentes para possibilitar uma an√°lise de dados qualificada.

Considere desenvolver seu c√≥digo localmente primeiro e com poucos dados para depois leva-lo para a AWS Lambda e aumentar a pesquisa de dados. APIs normalmente limitam requisi√ß√µes. Evite realizar muitas requisi√ß√µes em fase de desenvolvimento ou teste para evitarmos qualquer bloqueio na conta de voc√™s

Esta atividade corresponde a parte do desafio final. N√£o esperamos que voc√™ registre resposta neste espa√ßo. Contudo, deves adicionar o c√≥digo-fonte produzido ao seu reposit√≥rio no Github. Lembre-se de remover suas credenciais de acesso antes de efetuar commit.

Perguntas dessa tarefa:

Em sua conta AWS, no servi√ßo AWS Lambda, realize as seguintes atividades:

1.  Criar nova camada (layer) no AWS Lambda para as libs necess√°rias √† ingest√£o de dados.

2. Implementar o c√≥digo Python em AWS Lambda para consumo de dados do TMDB:

   - Buscar, pela API, os dados que complementem a an√°lise

   - Utilizar a lib boto3 para gravar os dados no AWS S3, em arquivos JSON, com 100 registros em cada arquivo.

    -----| no momento da grava√ß√£o dos dados deve-se considerar o padr√£o de path: <nome do bucket>\<camada de armazenamento>\<origem do dado>\<formato do dado>\<especifica√ß√£o do dado>\<data de processamento separada por ano\mes\dia>\<arquivo>

              S√£o exemplos de caminhos de arquivos v√°lidos:

               - S3:\\data-lake-do-fulano\raw\tmdb\json\2023\10\31\prt-uty-nfd.json

               - S3:\\data-lake-do-fulano\raw\tmdb\json\2023\10\31\idf-uet-wqt.json

Informa√ß√£o adicional:

Podemos utilizar os servi√ßos  CloudWatch Event ou Amazon EventBridge para agendar extra√ß√µes peri√≥dicas de dados no Twitter de forma autom√°tica.

**RESPOSTA:**

Inicialmente, foi feita uma confer√™ncia sobre a presen√ßa dos arquivos .CSV dentro do bucket S3, com os caminhos:

+ s3://compass.desafio/RAW/LOCAL/CSV/Movies/2023/10/23/movies.csv

+ s3://compass.desafio/RAW/LOCAL/CSV/Series/2023/10/23/series.csv

Ap√≥s isso, √© necess√°rio criar uma Fun√ß√£o Lambda e sua camada. Dessa forma, torna-se mais simples aplicar processamento aos dados quando eles s√£o inseridos ou deslocados na nuvem. 

![Fun√ß√£o Lambda](<Images Sprint8/Fun√ß√£o Lambda.png>)

A fun√ß√£o √© criada diretamente na plataforma da AWS, entretando a camada √© produzida localmente, utilizando do Docker para compactar os c√≥digos e dados adicionais. Seguimos os seguintes passos:

+ Criar o Dockerfile

```python
FROM amazonlinux:2.0.20200602.0
RUN yum update -y
RUN yum install -y \
python3-pip \
zip \
RUN yum -y clean all
RUN python3.7 -m pip install --upgrade pip
```

+ Aplicamos comandos ao terminal para criar uma imagem e bibliotecas

```python
docker build -t amazonlinuxpython37 .

docker run -it amazonlinuxpython37 bash

bash-4.2# cd ~
bash-4.2# mkdir layer_dir
bash-4.2# cd layer_dir/
bash-4.2# mkdir python
bash-4.2# cd python/
bash-4.2# pwd

pip install requests pandas urllib3==1.26.6 -t .

bash-4.2# cd ..
bash-4.2# zip -r minha-camada.zip .

docker container ls

docker cp <id do container>:/root/layer_dir/minha-camada.zip ./
```

![Dockerfile](<Images Sprint8/Dockerfile.png>)

Com o arquivo ZIP j√° pronto, criamos um novo bucket (tmdb_zip) e fazemos o upload deste. Retornamos ao Lambda, adicionando a camada orientada pelo caminho do bucket S3 (s3://tmdb.zip/minha-camada-pandas.zip).

![tmdb zip](<Images Sprint8/tmdb_zip.png>)

![tmdb zip 2](<Images Sprint8/tmdb_zip 2.png>)

Agora ser√° necess√°rio criar a fun√ß√£o lambda, onde nos permite a incorpora√ß√£o de l√≥gica personalizada em recursos da AWS, como os armazenamentos do Amazon S3 e as tabelas.


Este c√≥digo √© uma fun√ß√£o Lambda AWS que consome dados da API do The Movie Database (TMDB), armazena esses dados no Amazon S3 e realiza uma an√°lise para encontrar o filme com a maior m√©dia por d√©cada, registrando tamb√©m o n√∫mero de votos desse filme.

+ Ele come√ßa configurando o nome do bucket S3 onde os dados ser√£o armazenados e as chaves de API do TMDB.

+ Em seguida, ele faz uma consulta √† API do TMDB para obter dados de filmes do g√™nero "terror" (ou qualquer g√™nero especificado). Os par√¢metros da consulta s√£o definidos na vari√°vel params.

+ Os dados obtidos da API do TMDB s√£o armazenados na vari√°vel tmdb_data.

+ A data e hora atual s√£o usadas para criar um nome de arquivo no S3 para armazenar os dados do TMDB.

+ Os dados do TMDB s√£o armazenados no Amazon S3 usando a biblioteca boto3.

+ Ap√≥s armazenar os dados no S3, a fun√ß√£o analyze_tmdb_data √© chamada com os dados do TMDB para realizar a an√°lise.

+ A fun√ß√£o analyze_tmdb_data cria um dicion√°rio que armazena informa√ß√µes sobre os filmes com a maior m√©dia por d√©cada.

+ Ele itera pelos resultados da API do TMDB, calcula a d√©cada de lan√ßamento de cada filme e compara as m√©dias de votos para encontrar o filme com a maior m√©dia por d√©cada.

+ Os resultados da an√°lise s√£o armazenados em um formato JSON e tamb√©m s√£o enviados para o Amazon S3.

+ A fun√ß√£o retorna um objeto com um status de sucesso e uma mensagem informando que os dados do TMDB foram consumidos e a an√°lise foi armazenada no S3 com sucesso.

```python
import json
import requests
import boto3
from datetime import datetime

def get_movie_details(movie_id, api_key):
    movie_url = f'https://api.themoviedb.org/3/movie/{movie_id}?api_key={api_key}&language=en-US&append_to_response=revenue,budget'

    response = requests.get(movie_url)
    response.raise_for_status()  # Verifica se houve erro na requisi√ß√£o HTTP

    return response.json()

def get_movie_keywords(movie_id, api_key):
    keywords_url = f'https://api.themoviedb.org/3/movie/{movie_id}/keywords'
    params = {'api_key': api_key}

    response = requests.get(keywords_url, params=params)
    response.raise_for_status()  # Verifica se houve erro na requisi√ß√£o HTTP

    data = response.json()
    if 'keywords' in data:
        return [keyword['name'] for keyword in data['keywords']]
    else:
        return []

def lambda_handler(event, context):
    api_key = 'MINHA CHAVE'
    base_url = 'https://api.themoviedb.org/3/discover/movie'
    s3_bucket_name = 'compass.desafio'
    
    params_tmdb = {
        'api_key': api_key,
        'language': 'en-US',
        'sort_by': 'popularity.desc',
        'include_adult': 'false',
        'include_video': 'false',
        'page': 1
    }
    
    s3_client = boto3.client('s3')
    current_date = datetime.utcnow().strftime('%Y/%m/%d')
    s3_prefix = f'RAW/tmdb/json/{current_date}'
    
    current_page = 1
    while True:
        params_tmdb['page'] = current_page
        
        response = requests.get(base_url, params=params_tmdb)
        data = response.json()
        
        if 'results' not in data or not data['results']:
            break
        
        chunks = [data['results'][i:i + 100] for i in range(0, len(data['results']), 100)]
        
        for i, chunk in enumerate(chunks):
            for movie in chunk:
                movie_id = movie.get('id')
                if movie_id:
                    movie_details = get_movie_details(movie_id, api_key)
                    movie_keywords = get_movie_keywords(movie_id, api_key)

                    if movie_details is not None and movie_keywords is not None:
                        movie.update({
                            'revenue': movie_details.get('revenue'),
                            'budget': movie_details.get('budget'),
                            'keywords': movie_keywords
                        })

            s3_object_key = f'{s3_prefix}/prt-uty-nfd_{current_page}_{i + 1}.json'
            s3_object_url = f's3://{s3_bucket_name}/{s3_object_key}'
            
            s3_client.put_object(
                Bucket=s3_bucket_name,
                Key=s3_object_key,
                Body=json.dumps(chunk),
                ContentType='application/json'
            )
        
        current_page += 1
    
    return {
        'statusCode': 200,
        'body': json.dumps('Data successfully saved to S3!')
    }
```

# 3. ATIVIDADE 3 - Exerc√≠cios Gera√ß√£o de Massa de Dados

**1. [Warm up]  Em Python, declare e inicialize uma lista contendo 250 inteiros obtidos de forma aleat√≥ria. Ap√≥s, aplicar o m√©todo reverse sobre o conte√∫do da lista e imprimir o resultado.**

**RESPOSTA:**

<div align="center">

![Tarefa 3 1.2](<Images Sprint8/Tarefa 3 - 1.2.png>)

![Tarefa 3](<Images Sprint8/Tarefa 3 - 1.png>)

</div>

&nbsp;

**2. [Warm up] Em Python, declare e inicialize uma lista contendo o nome de 20 animais. Ordene-os em ordem crescente e itere sobre os itens, imprimindo um a um (voc√™ pode utilizar list comprehension aqui).  Na sequ√™ncia, armazene o conte√∫do da lista em um arquivo de texto, um item em cada linha, no formato CSV.**

```
animais = ['cachorro', 'gato', 'coelho', 'porco', 'galinha',

           'vaca', 'ovelha', 'elefante', 'girafa', 'tigre',

           'zebra', 'rinoceronte', 'macaco', 'canguru', 'urso',

           'lobo', 'raposa', 'panda', 'pinguim', 'crocodilo']

animais.sort()

[print(animal) for animal in animais]

with open("animais.csv", "w") as arquivo:

    for animal in animais:

        arquivo.write(animal + "\n")
```

**RESPOSTA:**

<div align="center">

![Tarefa 3 - 2](<Images Sprint8/Tarefa 3 - 2.1.png>)

![Tarefa 3 - 3](<Images Sprint8/Tarefa 3 - 2.2.png>)

![Animais](<Images Sprint8/animais.png>)

![Animais Open](<Images Sprint8/animais open.png>)

</div>

&nbsp;

**3. [Laborat√≥rio] Elaborar um c√≥digo Python para gerar um dataset de nomes de pessoas. Siga os passos a seguir para realizar a atividade:**

+ Passo 1:  Instalar biblioteca names para gera√ß√£o de nomes aleat√≥rios. O comando de instala√ß√£o √© pip install names

+ Passo 2 Importar as bibliotecas random, time, os e names em seu c√≥digo

+ Passo 3: Definir os par√¢metros para gera√ß√£o do dataset, ou seja, a quantidades de nomes aleat√≥rios e a quantidade de nomes que devem ser √∫nicos.

Define a semente de aleatoriedade

```python
random.seed(40)

qtd_nomes_unicos = 3000

qtd_nomes_aleatorios = 10000000
```

Nota: Quando trabalhamos com n√∫meros rand√¥micos em computa√ß√£o, na realidade, estamos falando de valores pseudoaleat√≥rios, uma vez que o computador n√£o consegue gerar n√∫meros verdadeiramente aleat√≥rios. No caso do Python, a fun√ß√£o random.seed inicializa o algoritmo respons√°vel pela gera√ß√£o de valores rand√¥micos. √â um processo determin√≠stico,  pois os valores gerados ser√£o sempre os mesmos se utilizado a mesma configura√ß√£o de inicializa√ß√£o. A este n√∫mero inicial chamamos de semente de aleatoriedade.

+ Passo 4: Gerar os nomes aleat√≥rios.

```python
aux=[]

for i in range(0, qtd_nomes_unicos):

    aux.append(names.get_full_name())

print("Gerando {} nomes aleat√≥rios".format(qtd_nomes_aleatorios))

dados=[]

for i in range(0,qtd_nomes_aleatorios):

    dados.append(random.choice(aux))
```

+ Passo 5: Gerar um arquivo de texto contendo todos os nomes, um a cada linha. O nome do arquivo deve ser nomes_aleatorios.txt

+ Passo 6: Abrir o arquivo e verificar seu conte√∫do (editor de texto)

**RESPOSTA:**

<div align="center">

![Tarefa 3 - 3.1](<Images Sprint8/Tarefa 3 - 3.1.png>)

![Tarefa 3 - 3.2](<Images Sprint8/Tarefa 3 - 3.2.png>)

![nomes](<Images Sprint8/nomes.png>)

![nomes open](<Images Sprint8/nomes open.png>)

</div>

&nbsp;

# 4. ATIVIDADE 4 - Exerc√≠cios Apache Spark

Neste laborat√≥rio voc√™ ir√° aplicar os recursos b√°sicos de manipula√ß√£o de dataframes atrav√©s do framework Apache Spark.

1. Inicialmente iremos preparar o ambiente, definindo o diret√≥rio onde nosso c√≥digo ser√° desenvolvido. Para este diret√≥rio iremos copiar o arquivo nomes_aleatorios.txt.

Ap√≥s, em nosso script Python, devemos importar as bibliotecas necess√°rias:

from pyspark.sql import SparkSession

from pyspark import SparkContext, SQLContext

Aplicando as bibliotecas do Spark, podemos definir a Spark Session e sobre ela definir o Context para habilitar o m√≥dulo SQL

spark = SparkSession \

                .builder \

                .master("local[*]")\

                .appName("Exercicio Intro") \

                .getOrCreate()

Nesta etapa, adicione c√≥digo para ler o arquivo nomes_aleatorios.txt atrav√©s do comando spark.read.csv. Carregue-o para dentro de um dataframe chamado df_nomes e, por fim, liste algumas linhas atrav√©s do m√©todo show. Exemplo: df_nomes.show(5)

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import rand, when, expr
 
# part 1
# Crie uma sess√£o Spark
spark = SparkSession.builder \
    .master("local[*]")\
    .appName("Exercicio Intro") \
    .getOrCreate()
 
# Leia o arquivo CSV
df_nomes = spark.read.csv("nomes_aleatorios.txt", header=False, inferSchema=True)
df_nomes = df_nomes.withColumnRenamed("_c0", "Nomes")
 
df_nomes.show(5)
```

<div align="center">

![Etapa 1 Output](<Images Sprint8/Atv4 Etp1.png>)

</div>

2. No Python, √© poss√≠vel acessar uma coluna de um objeto dataframe pelo atributo (por exemplo df_nomes.nome) ou por √≠ndice (df_nomes['nome']). Enquanto a primeira forma √© conveniente para a explora√ß√£o de dados interativos, voc√™ deve usar o formato de √≠ndice, pois caso algum nome de coluna n√£o esteja de acordo seu c√≥digo ir√° falhar.

Como n√£o informamos no momento da leitura do arquivo, o Spark n√£o identificou o Schema por padr√£o e definiu todas as colunas como string. Para ver o Schema, use o m√©todo df_nomes.printSchema().

Nesta etapa, ser√° necess√°rio adicionar c√≥digo para renomear a coluna para Nomes, imprimir o esquema e mostrar 10 linhas do dataframe.

```python
# part 2
# Mostre o esquema do DataFrame
df_nomes.printSchema()

# Mostre as 10 primeiras linhas do DataFrame
df_nomes.show(10)
```

<div align="center">

![Etapa 2](<Images Sprint8/Atv4 Etp2.png>)

</div>

3. Ao dataframe (df_nomes), adicione nova coluna chamada Escolaridade e atribua para cada linha um dos tr√™s valores de forma aleat√≥ria: Fundamental, Medio ou Superior.

Para esta etapa, evite usar fun√ß√µes de itera√ß√£o, como por exemplo: for, while, entre outras. D√™ prefer√™ncia aos m√©todos oferecidos para pr√≥prio Spark.

```python
# part 3
# Adicione a coluna "Escolaridade" de forma aleat√≥ria
df_nomes = df_nomes.withColumn("Escolaridade", when(rand() <= 0.33, "Fundamental")
                                   .when((rand() > 0.33) & (rand() <= 0.67), "M√©dio")
                                   .otherwise("Superior"))
df_nomes.show()
```

4. Ao dataframe (df_nomes), adicione nova coluna chamada Pais e atribua para cada linha o nome de um dos 13 pa√≠ses da Am√©rica do Sul, de forma aleat√≥ria.

Para esta etapa, evite usar fun√ß√µes de itera√ß√£o, como por exemplo: for, while, entre outras. D√™ prefer√™ncia aos m√©todos oferecidos para pr√≥prio Spark.

```python
# part 4
# Adicione a coluna "Pais" de forma aleat√≥ria
paises_sul_america = ["Argentina", "Brasil", "Chile", "Col√¥mbia", "Equador", "Paraguai", "Peru", "Uruguai", "Venezuela", "Bol√≠via", "Guiana", "Suriname", "Guiana Francesa"]
df_nomes = df_nomes.withColumn("Pais", when(rand() <= 0.1, "Argentina")
                                   .when(rand() <= 0.2, "Brasil")
                                   .when(rand() <= 0.3, "Chile")
                                   .when(rand() <= 0.4, "Col√¥mbia")
                                   .when(rand() <= 0.5, "Equador")
                                   .when(rand() <= 0.6, "Paraguai")
                                   .when(rand() <= 0.7, "Peru")
                                   .when(rand() <= 0.8, "Uruguai")
                                   .when(rand() <= 0.9, "Venezuela")
                                   .otherwise("Bol√≠via"))
```

5. Ao dataframe (df_nomes), adicione nova coluna chamada AnoNascimento e atribua para cada linha um valor de ano entre 1945 e 2010, de forma aleat√≥ria. 

Para esta etapa, evite usar fun√ß√µes de itera√ß√£o, como por exemplo: for, while, entre outras. D√™ prefer√™ncia aos m√©todos oferecidos para pr√≥prio Spark.

```python
# part 5
# Adicione a coluna "AnoNascimento" de forma aleat√≥ria
df_nomes = df_nomes.withColumn("AnoNascimento", expr("int(1945 + rand() * 66)"))
```

6. Usando o m√©todo select do dataframe (df_nomes), selecione as pessoas que nasceram neste s√©culo. Armazene o resultado em outro dataframe chamado df_select e mostre 10 nomes deste.

```python
# Selecione pessoas que nasceram neste s√©culo (a partir de 2000)
df_select = df_nomes.filter(df_nomes.AnoNascimento >= 2000)
df_select.show(10)
```

7. Usando Spark SQL repita o processo da Pergunta 6. Lembre-se que, para trabalharmos com SparkSQL, precisamos registrar uma tabela tempor√°ria e depois executar o comando SQL. Abaixo um exemplo de como executar comandos SQL com SparkSQL:

df_nomes.createOrReplaceTempView ("pessoas")

spark.sql("select * from pessoas").show()

```python
# part 6
# Registre o DataFrame como uma tabela tempor√°ria
df_nomes.createOrReplaceTempView("pessoas")
 
# Execute a consulta SQL para selecionar pessoas que nasceram neste s√©culo
query_sql = """
SELECT *
FROM pessoas
WHERE AnoNascimento >= 2000
"""
df_select_sql = spark.sql(query_sql)
df_select_sql.show(10)
```

8. Usando o m√©todo select do Dataframe df_nomes, Conte o n√∫mero de pessoas que s√£o da gera√ß√£o Millennials (nascidos entre 1980 e 1994) no Dataset

```python
# part 8
# Contar o n√∫mero de Millennials (nascidos entre 1980 e 1994)
df_millennials = df_nomes.filter((df_nomes.AnoNascimento >= 1980) & (df_nomes.AnoNascimento <= 1994))
count_millennials = df_millennials.count()
print(f"N√∫mero de Millennials: {count_millennials}")

9. Repita o processo da Pergunta 8 utilizando Spark SQL

```python
# part 9
# Execute a consulta SQL para contar o n√∫mero de Millennials
query_millennials = """
SELECT COUNT(*) AS CountMillennials
FROM pessoas
WHERE AnoNascimento >= 1980 AND AnoNascimento <= 1994
"""
 
millennials_count = spark.sql(query_millennials).first()[0]
print(f"N√∫mero de Millennials: {millennials_count}")
 
df_nomes.createOrReplaceTempView("pessoas")
```

10. Usando Spark SQL, obtenha a quantidade de pessoas de cada pa√≠s para uma das gera√ß√µes abaixo. Armazene o resultado em um novo dataframe e depois mostre todas as linhas em ordem crescente de Pais, Gera√ß√£o e Quantidade

- Baby Boomers ‚Äì nascidos entre 1944 e 1964;

- Gera√ß√£o X ‚Äì nascidos entre 1965 e 1979;4

- Millennials (Gera√ß√£o Y) ‚Äì nascidos entre 1980 e 1994;

- Gera√ß√£o Z ‚Äì nascidos entre 1995 e 2015.

```python
# part 10
# Defina as consultas SQL
queryBoomers = """
SELECT Pais, 'Baby Boomers' AS Geracao, COUNT(*) AS Quantidade
FROM pessoas
WHERE AnoNascimento >= 1944 AND AnoNascimento <= 1964
GROUP BY Pais
"""
 
queryGenX = """
SELECT Pais, 'Geracao X' AS Geracao, COUNT(*) AS Quantidade
FROM pessoas
WHERE AnoNascimento >= 1965 AND AnoNascimento <= 1979
GROUP BY Pais
"""
 
queryMillennials2 = """
SELECT Pais, 'Millennials' AS Geracao, COUNT(*) AS Quantidade
FROM pessoas
WHERE AnoNascimento >= 1980 AND AnoNascimento <= 1994
GROUP BY Pais
"""
 
queryGenZ = """
SELECT Pais, 'Geracao Z' AS Geracao, COUNT(*) AS Quantidade
FROM pessoas
WHERE AnoNascimento >= 1995 AND AnoNascimento <= 2015
GROUP BY Pais
"""
 
# Execute as consultas SQL
dfResultado = spark.sql(queryBoomers).union(spark.sql(queryGenX)).union(spark.sql(queryMillennials2)).union(spark.sql(queryGenZ))
dfResultado = dfResultado.orderBy("Pais", "Geracao", "Quantidade")
dfResultado.show()
```

# üìù Certifica√ß√£o

![Certificado Data Analytics 8/10](<Images Sprint8/Certificado Data Analytics 8-10.jpg>)
---
