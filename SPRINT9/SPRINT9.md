# üîñ **SUM√ÅRIO**

Nesta p√°gina encontra-se 4 T√≥picos Principais, s√£o estes:

+ **[TAREFA 1: Modelagem Relacional - Normaliza√ß√£o](#1-tarefa-1-modelagem-relacional---normaliza√ß√£o)**
+ **[TAREFA 2: Modelagem Dimensional - Cria√ß√£o de Modelo](#2-tarefa-2-modelagem-dimensional---cria√ß√£o-de-modelo)**
+ **[TAREFA 3: Desafio Parte 3 - Processamento da Trusted](#3-tarefa-3-desafio-parte-3---processamento-da-trusted)**
+ **[TAREFA 4: Desafio Parte 4 - Modelagem de Dados da Refined](#4-tarefa-4-desafio-parte-3---modelagem-de-dados-da-refined)**
+ **[TAREFA 5: Defasio Parte 5 - Processamento de Refined](#5-tarefa-5-desafio-parte-3---processamento-da-refined)**
+ **[Certifica√ß√£o](#üìù-certifica√ß√£o).**

---

&nbsp;

# 1. TAREFA 1: Modelagem Relacional - Normaliza√ß√£o

Um desafio pr√°tico relacionado a modelagem relacional, receber√£o um modelo desnormalizado para aplicarem as formas normais.

Aqui voc√™ ir√° praticar conceitos de Modelagem relacional que estudou anteriormente. Estaremos considerando a base de dados Concessionaria, cujo modelo √© apresentado na Modelagem L√≥gica abaixo.


O desafio √© normalizar esta base de dados, ou seja, aplicar as formas normais.

Observa√ß√µes: Para auxiliar na resolu√ß√£o, voc√™ poder√° baixar o arquivo contendo o banco de dados (concessionaria.zip) para seu computador, e, com aux√≠lio de algum cliente SQL, executar as queries localmente. Lembre-se de descompactar o arquivo antes.

S√£o exemplos de clientes SQL o DBeaver, SQLiteStudio e DataGrip. O primeiro costuma ser utilizado com maior frequ√™ncia.

<div align="center">

![Tb Loca√ß√£o](<Images Sprint9/tb_locacao.png>)

</div>

**Perguntas dessa tarefa:** Adicione sua resposta (formato .SQL) ao seu reposit√≥rio Git na respectiva Sprint e coloque abaixo o link do GitHub onde o(s) arquivos SQL est√£o salvos.

N√£o √© obrigat√≥rio, mas seria interessante:

- uma explica√ß√£o breve dos passos seguidos para a normaliza√ß√£o (aqui na janela abaixo)

- o desenho da Modelagem L√≥gica ap√≥s a normaliza√ß√£o. (anexado aqui abaixo)


**RESPOSTA:**

O exerc√≠cio envolveu a normaliza√ß√£o de uma base de dados para uma concession√°ria, originalmente apresentada em uma √∫nica tabela com diversos atributos. A normaliza√ß√£o √© um processo que visa organizar os dados de uma maneira eficiente, reduzindo a redund√¢ncia e melhorando a integridade referencial.

Etapas de Normaliza√ß√£o:

Identifica√ß√£o de Entidades:

A primeira etapa foi identificar as diferentes entidades envolvidas nos dados fornecidos. No seu caso, identificamos as entidades Combustiveis, Carros, Vendedores, e Locacoes.
Cria√ß√£o de Tabelas Separadas:

Cada entidade foi representada por uma tabela separada. Isso ajuda a evitar a repeti√ß√£o de dados e a manter a consist√™ncia.
Atribui√ß√£o de Chaves Prim√°rias e Estrangeiras:

Em cada tabela, uma coluna foi designada como chave prim√°ria (PK), que √© uma maneira √∫nica de identificar cada registro. Al√©m disso, foram definidas chaves estrangeiras (FK) para estabelecer rela√ß√µes entre as tabelas.
Refatora√ß√£o de Relacionamentos:

Os relacionamentos entre as entidades foram reformulados usando chaves estrangeiras. Por exemplo, na tabela Locacoes, foram adicionadas refer√™ncias para as chaves prim√°rias de Clientes, Carros, e Vendedores.
Elimina√ß√£o de Redund√¢ncias:

O processo de normaliza√ß√£o reduz a redund√¢ncia de dados, garantindo que cada informa√ß√£o seja armazenada em apenas um local. Isso ajuda a evitar inconsist√™ncias e economiza espa√ßo.
Melhoria na Manuten√ß√£o e Consist√™ncia:

Com as tabelas normalizadas, a manuten√ß√£o do banco de dados e a consist√™ncia dos dados se tornam mais f√°ceis de gerenciar. Altera√ß√µes em uma entidade n√£o afetam desnecessariamente outras partes do sistema.

Foi usada a seguinte l√≥gica em SQL:

```SQL
-- Tabela Clientes
CREATE TABLE Clientes (
    idCliente INT PRIMARY KEY,
    nomeCliente VARCHAR(100),
    cidadeCliente VARCHAR(40),
    estadoCliente VARCHAR(40),
    paisCliente VARCHAR(40)
);

-- Tabela Combustiveis
CREATE TABLE Combustiveis (
    idCombustivel INT PRIMARY KEY,
    tipoCombustivel VARCHAR(20)
);

-- Tabela Carros
CREATE TABLE Carros (
    idCarro INT PRIMARY KEY,
    kmCarro INT,
    chassisCarro VARCHAR(50),
    marcaCarro VARCHAR(80),
    modeloCarro VARCHAR(80),
    anoCarro INT,
    idCombustivel INT REFERENCES Combustiveis(idCombustivel)
);

-- Tabela Vendedores
CREATE TABLE Vendedores (
    idVendedor INT PRIMARY KEY,
    nomeVendedor VARCHAR(15),
    sexoVendedor SMALLINT,
    estadoVendedor VARCHAR(40)
);

-- Tabela Locacoes
CREATE TABLE Locacoes (
    idLocacao INT PRIMARY KEY,
    idCliente INT REFERENCES Clientes(idCliente),
    idCarro INT REFERENCES Carros(idCarro),
    dataLocacao DATETIME,
    horaLocacao TIME,
    qtdDiaria INT,
    valorDiaria DECIMAL(18, 2),
    dataEntrega DATE,
    horaEntrega TIME,
    idVendedor INT REFERENCES Vendedores(idVendedor)
);
```


<div align="center">

![Atv1 Part1](<Images Sprint9/Atv1_p1.png>)

![Atv1 Part2](<Images Sprint9/Atv1_p2.png>)

![Atv1 Part3](<Images Sprint9/Atv1_p3.png>)

</div>

# 2. TAREFA 2: Modelagem Dimensional - Cria√ß√£o de Modelo

A tarefa pr√°tica aqui √© utilizar o Modelo Relacional criado por voc√™ na tarefa anterior e com base nela, criar uma Modelagem Dimensional.

Aqui voc√™ ir√° praticar conceitos de Modelagem Dimensional que estudou anteriormente. Estaremos considerando a base de dados Concessionaria, cujo modelo ser√° o criado por voc√™s na se√ß√£o anterior (Modelagem Relacional).

O desafio √© montar o Modelo Dimensional com base no Modelo Relacional (normalizado - feito por voc√™s) na se√ß√£o anterior.

Observa√ß√µes:

Dica: Criar views para fatos e dimens√µes

**Perguntas dessa tarefa:** Adicione sua resposta (formato .SQL) ao seu reposit√≥rio Git na respectiva Sprint e coloque abaixo o link do GitHub onde o(s) arquivos SQL est√£o salvos.

N√£o √© obrigat√≥rio, mas seria interessante:

- o desenho da Modelagem Dimensional (anexado aqui abaixo)

**RESPOSTA:**

A modelagem dimensional √© uma t√©cnica usada no projeto de data warehouses para fornecer uma vis√£o mais intuitiva e eficiente dos dados. Nessa abordagem, voc√™ cria modelos que consistem em fatos e dimens√µes. Os fatos representam os n√∫meros que voc√™ deseja analisar, enquanto as dimens√µes s√£o as categorias pelas quais voc√™ deseja analisar esses fatos. 

Para a cria√ß√£o de tabelas, seguimos a seguinte l√≥gica:

```sql
-- Tabela de Fatos (FatoLocacao)
CREATE TABLE FatoLocacao (
    idFatoLocacao INT PRIMARY KEY,
    idCliente INT,
    idCarro INT,
    idVendedor INT,
    idDataTempo INT,
    qtdDiaria INT,
    valorDiaria DECIMAL(18, 2),
    dataEntrega DATE,
    horaEntrega TIME,
    FOREIGN KEY (idCliente) REFERENCES DimCliente(idCliente),
    FOREIGN KEY (idCarro) REFERENCES DimCarro(idCarro),
    FOREIGN KEY (idVendedor) REFERENCES DimVendedor(idVendedor),
    FOREIGN KEY (idDataTempo) REFERENCES DimDataTempo(idDataTempo)
);

-- Tabelas de Dimens√µes
CREATE TABLE DimCliente (
    idCliente INT PRIMARY KEY,
    nomeCliente VARCHAR(100),
    cidadeCliente VARCHAR(40),
    estadoCliente VARCHAR(40),
    paisCliente VARCHAR(40)
);

CREATE TABLE DimCarro (
    idCarro INT PRIMARY KEY,
    kmCarro INT,
    chassiCarro VARCHAR(50),
    marcaCarro VARCHAR(80),
    modeloCarro VARCHAR(80),
    anoCarro INT,
    idCombustivel INT,
    FOREIGN KEY (idCombustivel) REFERENCES DimCombustivel(idCombustivel)
);

CREATE TABLE DimVendedor (
    idVendedor INT PRIMARY KEY,
    nomeVendedor VARCHAR(15),
    sexoVendedor SMALLINT,
    estadoVendedor VARCHAR(40)
);

CREATE TABLE DimDataTempo (
    idDataTempo INT PRIMARY KEY,
    dataLocacao DATETIME,
    horaLocacao TIME
);

-- Tabela de Dimens√£o Adicional (DimCombustivel
CREATE TABLE DimCombustivel (
    idCombustivel INT PRIMARY KEY,
    tipoCombustivel VARCHAR(20)
);
```

<div align="center">

![Atv2 Part1](<Images Sprint9/Atv2_p1.png>)

![diagrama tarefa 2](<Images Sprint9/diagrama tarefa 2.png>)
</div>



# 3. TAREFA 3: Desafio Parte 3 - Processamento da Trusted

A camada Trusted de um data lake corresponde √†quela em que os dados encontram-se limpos e s√£o confi√°veis. √â resultado da integra√ß√£o das diversas fontes de origem, que encontram-se na camada anterior, que chamamos de Raw.

Aqui faremos uso do Apache Spark no processo, integrando dados existentes na camada Raw Zone. O objetivo √© gerar uma vis√£o padronizada dos dados, persistida no S3,  compreendendo a Trusted Zone do data lake.  Nossos jobs Spark ser√£o criados por meio do AWS Glue.

Todos os dados ser√£o persistidos na Trusted no formato PARQUET, particionados por data de coleta do TMDB (dt=<ano-m√™s-dia> exemplo: dt=2023-11-31). A exce√ß√£o fica para os dados oriundos do processamento batch (CSV), que n√£o precisam ser particionados.

Iremos separar o processamento em dois jobs: o primeiro, para carga hist√≥rica, ser√° respons√°vel pelo processamento dos arquivos CSV  e o segundo, para carga de dados do TMDB (e outra API, se utilizada). Lembre-se que suas origens ser√£o os dados existentes na RAW Zone.

Importante: Desenvolva os jobs no Glue utilizando a op√ß√£o Spark script editor.  Ap√≥s, na aba Job details, atente para as seguintes op√ß√µes:

Worker type: Informe G 1x (op√ß√£o de menor configura√ß√£o).

Requested  number of workers: Informe 2, que √© a quantidade m√≠nima.

Job timeout (minutes): Mantenha em 60 ou menos, se poss√≠vel.

Ap√≥s realizar a atividade, lembre-se de finalizar qualquer execu√ß√£o ativa de job para n√£o incorrer em custos desnecess√°rios.

**Perguntas dessa tarefa:** Realize as atividades conforme as instru√ß√µes. Neste espa√ßo, voc√™ pode adicionar prints dos jobs criados. Contudo, lembre-se de adicionar todo c√≥digo elaborado no seu reposit√≥rio do Github.

**RESPOSTA:**

JOB 1

```python
# Job1 Script (CSV to Parquet)

import sys
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_INPUT_PATH', 'S3_TARGET_PATH'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

source_path_csv = args['S3_INPUT_PATH']
target_path_csv = args['S3_TARGET_PATH']

df_csv = spark.read.csv(source_path_csv, sep="|", header=True)

df_csv.write.format("parquet").mode("append").save(target_path_csv)

job.commit()
```

<div align="center">

![Atv3 Part1](<Images Sprint9/Atv3_p1.png>)

![Atv3 Bucket 1](<Images Sprint9/Atv3 Bucket 1.png>)
</div>

JOB 2

```SQL
import sys
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions
from awsglue.job import Job

# Obter argumentos
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_INPUT_PATH', 'S3_TARGET_PATH'])

# Configurar o contexto do Spark e Glue
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Caminhos
source_path_tmdb = args['S3_INPUT_PATH']
target_path = args['S3_TARGET_PATH']

movies_df = spark.read.json(source_path_tmdb)

# Escrever DataFrame Spark no caminho de destino
movies_df.write.parquet(target_path, mode='overwrite')

# Finalizar o job
job.commit()
```

<div align="center">

![Atv3 Part2](<Images Sprint9/Atv3_p2.png>)

![Atv3 Bucket 2](<Images Sprint9/Atv3 Bucket 2.png>)

</div>

# 4. TAREFA 4: Desafio Parte 3 - Modelagem de dados da Refined

A camada Refined corresponde √† camada de um data lake em que os dados est√£o prontos para an√°lise e extra√ß√£o de insights. Sua origem corresponde aos dados da camada anterior, a Trusted.

Devemos pensar em estruturar os dados seguindo os princ√≠pios de modelagem multidimensional, a fim de permitir consultas sobre diferentes perspectivas.

Nesta etapa do desafio, voc√™ deve fazer a modelagem de dados da camada refined, definindo as tabelas e, se necess√°rio, views, a fim de disponibilizar os dados para a ferramenta de visualiza√ß√£o (QuickSight, a partir da pr√≥xima Sprint). Lembre-se que a origem ser√° os dados oriundos da Trusted Zone.

**Perguntas dessa tarefa:** Apresentar a modelagem de dados da camada Refined. Voc√™ pode exportar seu modelo de dados na forma de imagem e registrar aqui. Lembre-se de deix√°-lo dispon√≠vel tamb√©m no seu reposit√≥rio do Github.

**RESPOSTA:**

```SQL
-- Tabela Normalizada
CREATE TABLE tab_normalizada (
    id_filme INT PRIMARY KEY AUTO_INCREMENT,
    tituloprincipal VARCHAR(255) NOT NULL,
    anolancamento VARCHAR(4),
    tempominutos VARCHAR(5),
    genero VARCHAR(50),
    notamedia VARCHAR(5),
    quantidadevotos INT,
    mediavotos DOUBLE,
    popularidadefilme DOUBLE,
    orcamentofilme INT,
    receitafilme INT,
    palavrachave VARCHAR(255)
);

-- Tabela de Artistas
CREATE TABLE artista (
    id_artista INT PRIMARY KEY AUTO_INCREMENT,
    id_filme INT,
    generoartista VARCHAR(50),
    personagem VARCHAR(255),
    nomeartista VARCHAR(255),
    anonascimento VARCHAR(4),
    anofalecimento VARCHAR(4),
    profissao VARCHAR(50),
    FOREIGN KEY (id_filme) REFERENCES tab_normalizada(id_filme)
);

-- Tabela de Avalia√ß√£o
CREATE TABLE avaliacao (
    id_filme INT PRIMARY KEY AUTO_INCREMENT,
    notamedia VARCHAR(5),
    quantidadevotos INT,
    mediavotos DOUBLE,
    popularidadefilme DOUBLE,
    FOREIGN KEY (id_filme) REFERENCES tab_normalizada(id_filme)
);

-- Tabela de Fatos do Filme
CREATE TABLE fatos_filme (
    id_filme INT PRIMARY KEY AUTO_INCREMENT,
    tituloprincipal VARCHAR(255) NOT NULL,
    anolancamento VARCHAR(4),
    tempominutos VARCHAR(5),
    genero VARCHAR(50),
    palavrachave VARCHAR(255),
    FOREIGN KEY (id_filme) REFERENCES tab_normalizada(id_filme)
);

-- Tabela Financeira
CREATE TABLE financeiro (
    id_filme INT PRIMARY KEY AUTO_INCREMENT,
    orcamentofilme INT,
    receitafilme INT,
    FOREIGN KEY (id_filme) REFERENCES tab_normalizada(id_filme)
);
```

<div align="center">

![Diagrama tarefa 4](<Images Sprint9/diagrama tarefa 4.png>)

</div>

**Tabela de Fatos (tab_normalizada):**

id_filme: Identificador √∫nico para cada filme (chave prim√°ria).
tituloprincipal: T√≠tulo principal do filme.
anolancamento: Ano de lan√ßamento do filme.
tempominutos: Dura√ß√£o do filme em minutos.
genero: G√™nero do filme.
notamedia: Nota m√©dia atribu√≠da ao filme.
quantidadevotos: Quantidade de votos recebidos pelo filme.
mediavotos: M√©dia dos votos do filme.
popularidadefilme: Valor de popularidade associado ao filme.
orcamentofilme: Or√ßamento do filme.
receitafilme: Receita gerada pelo filme.
palavrachave: Palavras-chave associadas ao filme.
Tabelas de Dimens√£o:

**artista:**

id_artista: Identificador √∫nico para cada artista (chave prim√°ria).
id_filme: Chave estrangeira referenciando a tabela de fatos (tab_normalizada).
generoartista: G√™nero do artista.
personagem: Personagem interpretado pelo artista.
nomeartista: Nome do artista.
anonascimento: Ano de nascimento do artista.
anofalecimento: Ano de falecimento do artista.
profissao: Profiss√£o do artista.

**avaliacao:**

id_filme: Chave estrangeira referenciando a tabela de fatos (tab_normalizada).
notamedia: Nota m√©dia associada √† avalia√ß√£o do filme.
quantidadevotos: Quantidade de votos recebidos na avalia√ß√£o.
mediavotos: M√©dia dos votos recebidos na avalia√ß√£o.
popularidadefilme: Valor de popularidade associado √† avalia√ß√£o.

**fatos_filme:**

id_filme: Identificador √∫nico para cada filme (chave prim√°ria).
tituloprincipal: T√≠tulo principal do filme.
anolancamento: Ano de lan√ßamento do filme.
tempominutos: Dura√ß√£o do filme em minutos.
genero: G√™nero do filme.
palavrachave: Palavras-chave associadas ao filme.

**financeiro:**

id_filme: Chave estrangeira referenciando a tabela de fatos (tab_normalizada).
orcamentofilme: Or√ßamento do filme.
receitafilme: Receita gerada pelo filme.

**Relacionamentos:**

A tabela de fatos (tab_normalizada) est√° relacionada √†s tabelas de dimens√£o (artista, avaliacao, fatos_filme, financeiro) por meio de chaves estrangeiras, proporcionando uma estrutura multidimensional para an√°lises. Este modelo permite a an√°lise detalhada dos filmes a partir de diferentes perspectivas, considerando caracter√≠sticas como artistas, avalia√ß√µes, dados financeiros e informa√ß√µes espec√≠ficas do filme. Esse design facilita consultas anal√≠ticas em um ambiente de data lake.

&nbsp;

# 5. TAREFA 5: Desafio Parte 3 - Processamento da Refined

**Processamento de dados - Camada Refined**

Na atividade anterior, voc√™ definiu seu modelo de dados da camada Refined. Agora voc√™ deve processar os dados da camada Trusted, armazenando-os na Refined, de acordo com seu modelo.

Usaremos novamente do Apache Spark no processo, utilizando jobs cuja origem sejam dados da camada Trusted Zone e e o destino, a camada Refined Zone. Aqui, novamente, todos os dados ser√£o persistidos no formato PARQUET, particionados, se necess√°rio,  de acordo com as necessidades definidas para a camada de visualiza√ß√£o. Al√©m disso, √© necess√°rio registrar as tabelas geradas no AWS Glue Data Catalog para poder fazer consultas posteriormente.

Importante:

Desenvolva os jobs no Glue utilizando a op√ß√£o Spark script editor. Ap√≥s, na aba Job details, atente para as seguintes op√ß√µes:

+ Worker type: Informe G 1x (op√ß√£o de menor configura√ß√£o).

+ Requested number of workers: Informe 2, que √© a quantidade m√≠nima.

+ Job timeout (minutes): Mantenha em 60 ou menos, se poss√≠vel.

Ap√≥s realizar a atividade, lembre-se de finalizar qualquer execu√ß√£o ativa de job para n√£o incorrer em custos desnecess√°rios.

**Perguntas dessa tarefa:** Desenvolva os jobs de processamento de acordo com as instru√ß√µes. Aqui voc√™ pode apresentar prints do seu c√≥digo. Lembre-se tamb√©m de adicionar todo c√≥digo produzido ao seu reposit√≥rio no Github.

**RESPOSTA:**

<div align="center">

![crawlers desafio 3](<Images Sprint9/crawler desafio 3.png>)

![script refined](<Images Sprint9/script_refined.png>)

</div>

```python
import sys
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions
from awsglue.job import Job
from pyspark.sql import SparkSession
from pyspark.sql.functions import monotonically_increasing_id

args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_INPUT_PATH_CSV', 'S3_INPUT_PATH_TMDB', 'S3_TARGET_PATH'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

source_path_csv = args['S3_INPUT_PATH_CSV']
source_path_tmdb = args['S3_INPUT_PATH_TMDB']
target_path = args['S3_TARGET_PATH']

df_input_csv = spark.read.parquet(source_path_csv)
df_input_csv = df_input_csv.withColumn("id", monotonically_increasing_id())
df_input_csv.createOrReplaceTempView("csv")

df_input_tmdb = spark.read.parquet(source_path_tmdb)
df_input_tmdb = df_input_tmdb.withColumn("id", monotonically_increasing_id())
df_input_tmdb.createOrReplaceTempView("tmdb")

# Criar tabela tab_csv
consulta_tab_csv = """
SELECT DISTINCT
    tituloPincipal AS tituloPrincipal,
    anoLancamento,
    tempoMinutos,
    genero,
    notaMedia,
    numeroVotos,
    generoArtista,
    personagem,
    nomeArtista,
    anoNascimento,
    anoFalecimento,
    profissao
FROM csv
"""

df_result_tab_csv = spark.sql(consulta_tab_csv)
df_result_tab_csv.createOrReplaceTempView("tab_csv")

# Criar tabela tab_tmdb
consulta_tab_tmdb = """
SELECT DISTINCT
    CAST(id AS STRING) AS id_filme,
    title AS tituloPrincipal,
    popularity AS popularidadeFilme,
    vote_average AS mediaVotos,
    CAST(vote_count AS INT) AS quantidadeVotos,
    CAST(budget AS INT) AS orcamentoFilme,
    CAST(revenue AS INT) AS receitaFilme,
    CAST(keywords AS STRING) AS palavraChave
FROM tmdb
"""

df_result_tab_tmdb = spark.sql(consulta_tab_tmdb)
df_result_tab_tmdb.createOrReplaceTempView("tab_tmdb")

# Criar tabela tab_normalizada
consulta_normalizada = """
SELECT DISTINCT
    CAST(t2.id_filme AS STRING) AS id_filme,
    t1.tituloPrincipal,
    t1.anoLancamento,
    t1.tempoMinutos,
    t1.genero,
    t1.notaMedia,
    t1.numeroVotos,
    t1.generoArtista,
    t1.personagem,
    t1.nomeArtista,
    t1.anoNascimento,
    t1.anoFalecimento,
    t1.profissao,
    t2.popularidadeFilme,
    t2.mediaVotos,
    CAST(t2.quantidadeVotos AS INT) AS quantidadeVotos,
    CAST(t2.orcamentoFilme AS INT) AS orcamentoFilme,
    CAST(t2.receitaFilme AS INT) AS receitaFilme,
    CAST(t2.palavraChave AS STRING) AS palavraChave
FROM tab_csv t1
INNER JOIN tab_tmdb t2 ON t1.tituloPrincipal = t2.tituloPrincipal
"""

df_result_normalizada = spark.sql(consulta_normalizada)
df_result_normalizada.createOrReplaceTempView("tab_normalizada")

# Criar tabela fatos_filme
consulta_fatos_filme = """
SELECT
    id_filme,
    tituloPrincipal,
    anoLancamento,
    tempoMinutos,
    genero,
    palavraChave
FROM tab_normalizada
"""

df_result_fatos_filme = spark.sql(consulta_fatos_filme)
df_result_fatos_filme.createOrReplaceTempView("fatos_filme")

# Criar tabela financeiro
consulta_financeiro = """
SELECT
    id_filme,
    orcamentoFilme,
    receitaFilme
FROM tab_normalizada
"""

df_result_financeiro = spark.sql(consulta_financeiro)
df_result_financeiro.createOrReplaceTempView("financeiro")

# Criar tabela avaliacao
consulta_avaliacao = """
SELECT
    id_filme,
    notaMedia,
    quantidadeVotos,
    mediaVotos,
    popularidadeFilme
FROM tab_normalizada
"""

df_result_avaliacao = spark.sql(consulta_avaliacao)
df_result_avaliacao.createOrReplaceTempView("avaliacao")

# Criar tabela artista
consulta_artista = """
SELECT
    id_filme,
    generoArtista,
    personagem,
    nomeArtista,
    anoNascimento,
    anoFalecimento,
    profissao
FROM tab_normalizada
"""

df_result_artista = spark.sql(consulta_artista)
df_result_artista.createOrReplaceTempView("artista")

# Escrever as tabelas no S3
df_result_tab_csv.write.format("parquet").mode("overwrite").save(f"{target_path}/tab_csv/")
df_result_tab_tmdb.write.format("parquet").mode("overwrite").save(f"{target_path}/tab_tmdb/")
df_result_normalizada.write.format("parquet").mode("overwrite").save(f"{target_path}/tab_normalizada/")
df_result_fatos_filme.write.format("parquet").mode("overwrite").save(f"{target_path}/fatos_filme/")
df_result_financeiro.write.format("parquet").mode("overwrite").save(f"{target_path}/financeiro/")
df_result_avaliacao.write.format("parquet").mode("overwrite").save(f"{target_path}/avaliacao/")
df_result_artista.write.format("parquet").mode("overwrite").save(f"{target_path}/artista/")

job.commit()

```

<div align="center">

![parametros](<Images Sprint9/parametros.png>)

</div>

+ **--S3_INPUT_PATH_CSV:** s3://compass.desafio3/Trusted/TMDBProcessed/CSV/

+ **--S3_INPUT_PATH_TMDB:** s3://compass.desafio3/Trusted/TMDBProcessed/TMDB/

+ **--S3_TARGET_PATH:** s3://compass.desafio3/Refined/lab_glue/tmdb_refined_crawler/

<div align="center">

![S3 Refined](<Images Sprint9/S3 refined.png>)

</div>

&nbsp;

# üìù Certifica√ß√£o

+ Certifica√ß√£o da conclu√ß√£o do curso Data & Analytics AWS 9/10.

<div align="center">

![Certificado Data & Analytics AWS 9/10](<Images Sprint9/Certificado.jpg>)

</div>

---