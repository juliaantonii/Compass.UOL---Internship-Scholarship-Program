# üîñ **SUM√ÅRIO**

Nesta p√°gina encontra-se 4 T√≥picos Principais, s√£o estes:

+ **[Hadop, MapReduce for Big Data Problems](#1-hadop-mapreduce-for-big-data-problems);**
+ **[Forma√ß√£o Spark com Pyspark: Curso Completo](#2-forma√ß√£o-spark-com-pyspark-curso-completo);**
+ **[Exerc√≠cios - Laborat√≥rios AWS](#exerc√≠cios---laborat√≥rios-aws);**
  + **[Tarefa 1: Python com Pandas e Numpy](#tarefa-1-python-com-pandas-e-numpy);**
  + **[Tarefa 2: Apache Spark - Contador de Palavras](#tarefa-2-apache-spark---contador-de-palavras);**
  + **[Lab AWS Glue](#lab-aws-glue);**
  + **[Tarefa 3: Desafio Parte 1 - ETL](#tarefa-3-desafio-parte-1---etl);**
+ **[Certifica√ß√£o](#üìù-certifica√ß√£o).**

---

&nbsp;

# 1. Hadop, MapReduce for Big Data Problems

Hadoop √© um framework de c√≥digo aberto projetado para armazenar e processar dados em grande escala (big data) em clusters de servidores. Foi desenvolvido pela Apache Software Foundation e √© amplamente utilizado em todo o mundo para gerenciar e analisar grandes volumes de dados. Hadoop √© composto por v√°rios componentes-chave que trabalham juntos para gerenciar eficazmente dados em grande escala. Aqui est√° uma explica√ß√£o dos principais componentes do Hadoop:

+ **Hadoop Distributed File System (HDFS):** O HDFS √© o sistema de arquivos distribu√≠do do Hadoop. Ele foi projetado para armazenar grandes arquivos de dados de forma redundante em v√°rios n√≥s (servidores) de um cluster. O HDFS divide os arquivos em blocos menores (normalmente de 128 MB a 256 MB) e os distribui pelos n√≥s do cluster. Isso permite alta escalabilidade e alta disponibilidade dos dados.

+ **MapReduce:** O MapReduce √© um modelo de programa√ß√£o e um sistema de processamento em lote que permite o processamento de dados distribu√≠dos em um cluster Hadoop. Ele funciona em duas etapas: a fase "Map" para o processamento inicial dos dados, seguida pela fase "Reduce" para agregar os resultados. O MapReduce permite a paraleliza√ß√£o eficiente do processamento de dados.

+ **YARN (Yet Another Resource Negotiator):** O YARN √© o gerenciador de recursos do Hadoop. Ele gerencia a aloca√ß√£o de recursos (CPU, mem√≥ria, etc.) para as tarefas em execu√ß√£o no cluster. O YARN substituiu o gerenciador de recursos originalmente usado pelo MapReduce, oferecendo maior flexibilidade para executar diversas cargas de trabalho no mesmo cluster.

+ **Hadoop Common:** Este √© um conjunto de bibliotecas e utilit√°rios compartilhados usados pelos outros componentes do Hadoop. Ele inclui ferramentas para gerenciar o cluster, bibliotecas para manipular arquivos e logs, bem como ferramentas de monitoramento de desempenho.

+ **Ecossistema Hadoop:** Al√©m desses componentes principais, o ecossistema Hadoop inclui muitos projetos e ferramentas adicionais para lidar com diferentes tipos de dados e executar tarefas diversas, como Hive (processamento de dados semelhante a SQL), Pig (linguagem de script para processamento de dados), HBase (banco de dados NoSQL), Spark (processamento de dados em mem√≥ria) e muito mais.

O Hadoop √© especialmente adequado para o processamento de dados em grande escala, como an√°lise de logs, minera√ß√£o de dados, processamento de dados geoespaciais e outras aplica√ß√µes que exigem alta escalabilidade e toler√¢ncia a falhas. Ele foi amplamente adotado em diversos setores, incluindo empresas, pesquisa, m√≠dias sociais e outros, para resolver problemas relacionados a big data.

<div align="center">

![Hadoop](<Images Sprint7/Hadoop.png>)

</div>

# 2. Forma√ß√£o Spark com Pyspark: Curso Completo

Spark √© um framework de c√≥digo aberto amplamente utilizado para processamento de dados em grande escala e an√°lise de big data. Ele foi desenvolvido para ser r√°pido, flex√≠vel e escal√°vel, sendo projetado para lidar com uma variedade de tipos de tarefas de processamento de dados, incluindo processamento em lote, processamento em tempo real, aprendizado de m√°quina e an√°lise interativa.

Aqui est√£o algumas caracter√≠sticas-chave do Spark:

+ **Velocidade:** O Spark √© conhecido por sua velocidade. Ele mant√©m os dados em mem√≥ria sempre que poss√≠vel, o que reduz significativamente o tempo de acesso aos dados. Isso o torna muito mais r√°pido do que sistemas de processamento de big data tradicionais, como o Hadoop MapReduce.

+ **Flexibilidade:** O Spark oferece suporte a v√°rias linguagens de programa√ß√£o, incluindo Scala, Python, Java e R. Isso permite que os desenvolvedores escolham a linguagem com a qual se sintam mais confort√°veis.

+ **Escalabilidade:** O Spark √© altamente escal√°vel e pode ser usado em clusters de servidores para processar grandes volumes de dados de forma distribu√≠da. Ele √© projetado para dimensionar horizontalmente, o que significa que voc√™ pode adicionar mais m√°quinas ao cluster conforme suas necessidades de processamento crescem.

+ **Suporte a Diversos Workloads:** O Spark suporta uma ampla gama de workloads, incluindo processamento em lote (usando o Spark Core e o Spark SQL), streaming em tempo real (usando o Spark Streaming), an√°lise de gr√°ficos (usando o GraphX) e aprendizado de m√°quina (usando o MLlib).

Agora, o PySpark √© a interface Python para o Apache Spark. Ele permite que os desenvolvedores usem a pot√™ncia do Spark em seus programas Python. Com o PySpark, voc√™ pode escrever aplicativos Spark usando Python, aproveitando todas as capacidades de processamento e an√°lise distribu√≠da do Spark.

Algumas caracter√≠sticas do PySpark:

+ **API Python:** O PySpark oferece uma API Python completa para interagir com o Spark, o que o torna uma op√ß√£o conveniente para desenvolvedores que est√£o familiarizados com Python.

+ **Integra√ß√£o com Bibliotecas Python:** Voc√™ pode facilmente integrar bibliotecas Python populares, como NumPy, Pandas e scikit-learn, com o PySpark para realizar an√°lises de dados mais avan√ßadas e processamento de machine learning.

+ **Acesso a todas as funcionalidades do Spark:** Com o PySpark, voc√™ pode acessar todas as funcionalidades e componentes do Spark, incluindo Spark SQL, Spark Streaming e MLlib.

Em resumo, o Spark √© um poderoso framework de processamento de big data, enquanto o PySpark √© a interface Python que permite que voc√™ aproveite o Spark usando a linguagem de programa√ß√£o Python. Juntos, eles oferecem uma solu√ß√£o flex√≠vel e eficaz para processamento e an√°lise de grandes volumes de dados.

# Exerc√≠cios - Laborat√≥rios AWS

## Tarefa 1: Python com Pandas e Numpy

Nesta tarefa iremos utilizar as bibliotecas Pandas e NumPy para responder a quatro exerc√≠cios. Lembre-se de adicionar o c√≥digo desenvolvido ao seu reposit√≥rio no GitHub para que o monitor(a) da Sprint possa avali√°-lo posteriormente.

<div align="center">

![Spark](<Images Sprint7/Spark.png>)

![Spark Actors](<Images Sprint7/Spark Actors.png>)

</div>

_**1. Identifique o ator/atriz com maior n√∫mero de filmes e o respectivo n√∫mero de filmes.**_

_**Resposta:**_ O ator/atriz com maior n√∫mero de filmes √© Robert DeNiro com 79 filmes.

<div align="center">

![T1 Ex1](<Images Sprint7/T1Ex1.png>)

</div>

_**2. Apresente a m√©dia da coluna contendo o n√∫mero de filmes.**_

_**Resposta:**_ A m√©dia do n√∫mero de filmes √© 37.88.

<div align="center">

![T1 Ex2](<Images Sprint7/T1Ex2.png>)

</div>

_**3. Apresente o nome do ator/atriz com a maior m√©dia por filme.**_

_**Resposta:**_ O ator/atriz com maior m√©dia por filme √© Anthony Daniels com 451.8 de M√©dia.

<div align="center">

![T1 Ex3](<Images Sprint7/T1Ex3.png>)

</div>

_**4. Apresente o nome do(s) filme(s) mais frequente(s) e sua respectiva frequ√™ncia.**_

_**Resposta:**_ Ranking dos 5 filmes de acordo com a frequ√™ncia no dataset

1 - O filme The Avengers aparece 6 vez(es) no dataset

2 - O filme Harry Potter / Deathly Hallows (P2) / aparece 4 vez(es) no dataset

3 - O filme Catching Fire aparece 4 vez(es) no dataset

4 - O filme The Dark Knight aparece 3 vez(es) no dataset

5 - O filme Star Wars: The Force Awakens aparece 3 vez(es) no dataset

<div align="center">

![T1 Ex4](<Images Sprint7/T1Ex4.png>)

</div>

&nbsp;

## Tarefa 2: Apache Spark - Contador de Palavras

Neste atividade voc√™ ir√° desenvolver um job de processamento com o framework Spark por meio de um container Docker. Lembre-se que seu c√≥digo-fonte dever√° estar no GitHub para posterior avalia√ß√£o do monitor(a) da Sprint.

_**1. Realizar o pull da imagem jupyter/all-spark-notebook e Criar um container a partir da imagem.**_

_**Resposta:**_

<div align="center">

![Container T2](<Images Sprint7/ContainerT2.png>)

![Jup Img T2](<Images Sprint7/Jup Img T2.png>)

![Pyspark](<Images Sprint7/Pyspark.png>)

![Jupyter Lab](<Images Sprint7/JupyterLab.png>)

</div>

_**3 - Em outro terminal, execute o comando `pyspark` no seu container. Pesquise sobre o comando  docker exec para realizar esta a√ß√£o. Utilize as flags -i e -t no comando.**_

_**Resposta:**_ O README.md usado para teste de leitura foi o [Contador de Palavras](https://github.com/juliaantonii/Contador-de-Palavras).

<div align="center">

![Exec Pyspark](<Images Sprint7/Exec Pyspark.png>)

![Comandos](<Images Sprint7/Comandos.png>)

![Comandos Usados](<Images Sprint7/Comandos Usados.png>)

![Contagem de Palavras](<Images Sprint7/Contagem de Palavras.png>)

</div>

## Lab AWS Glue

Processos de ETL (Extract, Transform and Load) est√£o presentes em todos os projetos de dados. O cen√°rio costuma ser o mesmo: fontes de dados diversas com datasets de interesse que precisam ser ingeridos, transformados e armazenados em um ou mais destinos, com formatos diferentes da origem.

Neste laborat√≥rio voc√™ ser√° guiado na constru√ß√£o de um processo de ETL simplificado utilizando o
servi√ßo AWS Glue.

_**1. Preparando os dados de origem**_

<div align="center">

![Bucket Compass Lab](<Images Sprint7/Bucket Compass Lab.png>)

</div>

_**2. Criando a IAM Role para os jobs do AWS Glue**_

<div align="center">

![AWS Glue Role](<Images Sprint7/AWSGlueRole.png>)

</div>

_**3 - Configurando as permiss√µes no AWS Lake Formation**_

![Database1](<Images Sprint7/Database1.png>)

![Database2](<Images Sprint7/Database2.png>)

![Database3](<Images Sprint7/Database3.png>)

</div>

_**4. Criando novo job no AWS Glue**_

<div align="center">

![Job 1](<Images Sprint7/Job1.png>)

![Job 2](<Images Sprint7/Job2.png>)

![Job 3](<Images Sprint7/Job3.png>)

</div>

_**5. Sua vez!**_

<div align="center">

![Script](<Images Sprint7/Script.png>)

</div>

_**6 - Criando novo crawler**_

<div align="center">

![Crawlers](<Images Sprint7/Crawlers.png>)

![Tables](<Images Sprint7/Tables.png>)

![Frequencia Registro Nomes EUA 1](<Images Sprint7/Frequencia Registro Nomes EUA 1.png>)

![Frequencia Registro Nomes EUA 2](<Images Sprint7/Frequencia Registro Nomes EUA 2.png>)

![Query](<Images Sprint7/Query Describe 1.png>)

![SQL Permitions](<Images Sprint7/SQL.png>)

</div>

## Tarefa 3: Desafio Parte 1 - ETL

Ingest√£o Batch: a ingest√£o dos arquivos CSV em Bucket Amazon S3 RAW Zone. Nesta etapa do desafio deve ser constru√≠do um c√≥digo Python que ser√° executado dentro de um container Docker para carregar os dados locais dos arquivos para a nuvem. Nesse caso utilizaremos, principalmente, as lib boto3 como parte do processo de ingest√£o via batch para gera√ß√£o de arquivo (CSV).

_**1. Implementar c√≥digo Python:**_

+ ler os 2 arquivos (filmes e series) no formato CSV inteiros, ou seja, sem filtrar os dados

+ utilizar a lib boto3 para carregar os dados para a AWS

+ acessar a AWS e grava no S3, no bucket definido com RAW Zone

    + no momento da grava√ß√£o dos dados deve-se considerar o padr√£o: <nome do bucket>\<camada de armazenamento>\<origem do dado>\<formato do dado>\<especifica√ß√£o do dado>\<data de processamento separada por ano\mes\dia>\<arquivo>
    Por exemplo:

                   S3:\\data-lake-do-fulano\Raw\Local\CSV\Movies\2022\05\02\movies.csv

                   S3:\\data-lake-do-fulano\Raw\Local\CSV\Series\2022\05\02\series.csv

_**Resposta:**_

![Script T3](<Images Sprint7/Script T3.png>)

_**2. Criar container Docker com um volume para armazenar os arquivos CSV e executar processo Python implementado**_

<div align="center">

![Dockerfile](<Images Sprint7/Dockerfile.png>)

![Container T3](<Images Sprint7/Container T3.png>)

![Docker Run](<Images Sprint7/Docker Run.png>)

![Docker Run2](<Images Sprint7/Docker Run2.png>)

![Leitura de Movies](<Images Sprint7/Leitura Movies.png>)

![Leitura de Series](<Images Sprint7/Leitura Series.png>)

</div>

&nbsp;

# üìù Certifica√ß√£o

+ Certifica√ß√£o da conclu√ß√£o do curso Forma√ß√£o Spark com Pyspark de 11hrs.

<div align="center">

![Certificado Forma√ß√£o Spark com Pyspark](<Images Sprint7/Certificado Forma√ß√£o Spark com Pyspark.jpg>)

</div>

+ Certifica√ß√£o da conclu√ß√£o do curso Learn By Example: Hadoop, MapReduce for Big Data problems de 14hrs.

<div align="center">

![Certificado Hadoop](<Images Sprint7/Certificado Hadoop.png>)

</div>

---